<html><body><p>After describing the project I'm working on to a few people at PyConAU and BrisPy, I decided it might be a good idea to blog about it here. I do have a bit of an ulterior motive in doing so, though - I hope people will point out when I've missed useful external resources or applications, or when something I'm planning to do doesn't make sense to the assorted Django developers I know. Yes, that's right - I'd like to make being <a href="https://www.xkcd.com/386/">wrong on the internet</a> work in my favour :)<br><br>The project is purely internal at this stage, but I hope to be able to publish it as open source somewhere down the line. Even being able to post these design concepts is pretty huge for me personally, though - before starting with Red Hat a few months ago, I spent the previous 12 and a half years working in the defence industry, which is about as far from Red Hat's <a href="http://draft.blogger.com/goog_2071189574">"</a><a href="http://draft.blogger.com/goog_2071189574">Default to Open" philosophy</a> as it's possible to get.<br><br></p><h2>  Mirror, Mirror, On The Wall</h2>The project Red Hat hired me to implement is the next generation of their internal mirroring system, which is used for various tasks, such as getting built versions of RHEL out to the hardware compatibility testing labs (and, when they're large enough, returning the generated log files to the relevant development sites), or providing internal Fedora mirrors at the larger Red Hat offices (such as the one here in Brisbane).<br><br>There are various use cases and constraints that mean the mirroring system needs to operate at the filesystem level without making significant assumptions about the contents of the trees being mirrored (due to various details of the use cases involved, block level replication and approaches that rely on the transferred data being laid out in specific ways aren't viable alternatives for this project). The current incarnation of this system relies almost entirely on that venerable workhorse of the mirroring world, <span>rsync</span>.<br><br>However, the current system is also showing its age and has a few limitations that make it fairly awkward to work with. Notably, there's no one place to go to get an overview of the entire internal mirroring setup, and the direct use of <span>rsync</span> means it isn't particularly friendly with other applications when it comes to sharing WAN bandwidth and the servers involved are wasting quite a few cycles recalculating the same deltas for multiple clients. Hence, the project I am working on, which is intended to replace the existing system with something a bit more efficient and easier to manage, while also providing a better platform for adding new features.<br><br><h2>  Enter Pulp</h2><a href="http://pulpproject.org/">Pulp</a> is an open source (Python) project created by Red Hat to make it easier to manage private <span>yum</span> repositories. Via <a href="http://katello.org/">Katello</a>, Pulp is one of the upstream components for Red Hat's <a href="https://www.redhat.com/solutions/cloud/cloudforms/">CloudForms</a> product.<br><br>The Pulp project is currently in the process of migrating from their original yum-specific architecture to a more general purpose <a href="http://blog.pulpproject.org/2011/09/06/pulp-rearchitecture-sprint-update/">Generic Content plugin architecture</a>. It's that planned plugin architecture that makes Pulp a useful basis for the next generation internal mirroring system, which, at least for now, I am imaginatively calling <span>pulpdist</span> (referring to both "distribution with Pulp", since that's what the system does, and "distributed Pulp instances", since that's how the system will work).<br><br>The main components of the initial <span>pulpdist</span> architecture will be:<br><ul><li>a front-end (Django 1.3) web app providing centralised management of the entire distribution network</li><li>custom importer and distributor plugins for Pulp to handle distribution of tree changes within the distribution network</li><li>custom importer plugins to handle the import of  trees from their original sources and generation of any additional metadata needed by the internal distribution plugins</li><li>generic (and custom, if needed) plugins to make the trees available to the applications that need them</li></ul><br>I'll be writing more on various details that I consider interesting as I go along. Initially, that will include my plan for the mirroring protocol to be used between the sites, as well as various decisions that need to be made when spinning up a Django project from scratch (while many of my specific answers are shaped by the target environment for internal deployment, the questions I needed to consider should be fairly widely applicable).</body></html>